{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(path, names):\n",
    "    if not path.is_file():\n",
    "        raise FileNotFoundError(str(path))\n",
    "        \n",
    "    data = pd.read_csv(path, sep=\",\", names=names, header=None)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_dfs():\n",
    "    cols = [\"PregnanciesNumber\", \"GlucosePlasma\", \"BloodPressureDiastolic\", \"SkinThicknessTriceps\", \n",
    "           \"Insulin2Hour\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"OutcomeClass\"]\n",
    "    path = Path.cwd() / \"data\"\n",
    "    diabetes_file = path / \"pima-indians-diabetes.data.csv\"\n",
    "    train_file = path / \"train.csv\"\n",
    "    test_file = path / \"test.csv\"\n",
    "\n",
    "    diabetes_data = load_file(diabetes_file, cols)\n",
    "    train_data = load_file(train_file, cols)\n",
    "    test_data = load_file(test_file, cols)\n",
    "    return diabetes_data, train_data, test_data\n",
    "\n",
    "\n",
    "def mean_and_std(data):\n",
    "    return data.mean(), data.std()\n",
    "\n",
    "\n",
    "def norm_dist(data, mean, std):\n",
    "    variance = std**2\n",
    "    denominator = (2 * math.pi* variance)**(.5)\n",
    "    numerator = np.exp(-(data - mean)**2 / (2 * variance))\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "def class_probs(data):\n",
    "    n_false = train_data['OutcomeClass'][train_data['OutcomeClass'] == 0].count()\n",
    "    n_true = train_data['OutcomeClass'][train_data['OutcomeClass'] == 1].count()\n",
    "    n_total = train_data['OutcomeClass'].count()\n",
    "\n",
    "    p_false = n_false / n_total\n",
    "    p_true = n_true / n_total\n",
    "    \n",
    "    return p_false, p_true\n",
    "\n",
    "\n",
    "def mean_std_by_class(data):\n",
    "    data_by_class = data.groupby('OutcomeClass')\n",
    "    mean, std = mean_and_std(data_by_class)\n",
    "    false_mean = mean[std.index == 0.0].values[0]\n",
    "    false_std = std[std.index == 0.0].values[0]\n",
    "    true_mean = mean[std.index == 1.0].values[0]\n",
    "    true_std = std[std.index == 1.0].values[0]\n",
    "    return false_mean, false_std, true_mean, true_std\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_data, train_data, test_data = load_dfs()\n",
    "\n",
    "p_false, p_true = class_probs(train_data)\n",
    "\n",
    "false_mean, false_std, true_mean, true_std = mean_std_by_class(train_data)\n",
    "\n",
    "test_data_no_outcome = test_data.drop('OutcomeClass', axis=1)\n",
    "\n",
    "false_norm = norm_dist(test_data_no_outcome, false_mean, false_std)\n",
    "false_norm = false_norm.prod(axis=1) * p_false\n",
    "\n",
    "true_norm = norm_dist(test_data_no_outcome, true_mean, true_std)\n",
    "true_norm = true_norm.prod(axis=1) * p_true\n",
    "\n",
    "norm = pd.concat([false_norm, true_norm], axis=1)\n",
    "\n",
    "norm['diabetes_predicted'] = np.where(norm[1] > norm[0], 1.0, 0.0)\n",
    "\n",
    "merged = pd.concat([norm['diabetes_predicted'], test_data['OutcomeClass']], axis=1)\n",
    "\n",
    "\n",
    "merged['accurate'] = np.where(merged['diabetes_predicted'] == merged['OutcomeClass'], True, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7480314960629921"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = merged.mean()['accurate']\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#adding colum true negative, false negative, true positive, and false positive and labeling with 1 label applies \n",
    "merged['TN'] = np.where((merged['diabetes_predicted']==0) & (merged['OutcomeClass']==0),1,0) \n",
    "merged['FN'] = np.where((merged['diabetes_predicted']==0) & (merged['OutcomeClass']==1),1,0)\n",
    "merged['FP'] = np.where((merged['diabetes_predicted']==1) & (merged['OutcomeClass']==0),1,0)\n",
    "merged['TP'] = np.where((merged['diabetes_predicted']==1) & (merged['OutcomeClass']==1),1,0)\n",
    "#merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Predicted: 0  Predicted: 1\n",
      "Actual: 0           132            28\n",
      "Actual: 1            36            58\n",
      "\n",
      "accuracy: 0.7480314960629921\n",
      "error: 0.25196850393700787\n",
      "sensitivity: 0.6170212765957447\n",
      "specificity: 0.825\n"
     ]
    }
   ],
   "source": [
    "#making dataFram with TN, FN, FP, TP only columns and find the sum of those columns\n",
    "confusion = merged.drop(['OutcomeClass','diabetes_predicted','accurate'],axis=1)\n",
    "confusion = confusion.sum(axis=0)\n",
    "#confusion\n",
    "\n",
    "#getting values from dataframe \n",
    "TN = confusion[0]\n",
    "FN = confusion[1]\n",
    "FP = confusion[2]\n",
    "TP = confusion[3]\n",
    "#calculating accuracy, error, senstivity and specificity\n",
    "#accuracy is the percent of times that we have predicted correct\n",
    "accuracy = (TP + TN)/(TP + FP + TN + FN)\n",
    "#error is the percent of times that we have predicted incorrect\n",
    "error = (FP + FN)/(TP + FP + TN + FN)\n",
    "#sensitivity tells us for each time that we predicted positive how many of those times were correct, in this case 61.7 percent\n",
    "sensitivity = (TP)/(FN + TP)\n",
    "#specificity tells us for each time that we predicted negative how many of those times were correct, in this case 82.5 percent\n",
    "specificity = TN/(TN + FP)\n",
    "\n",
    "#making a datafram to dispaly confusion matrix\n",
    "displayCMatrix = [[TN,FP],[FN,TP]] \n",
    "displayCMatrix = pd.DataFrame(displayCMatrix, columns = ['Predicted: 0','Predicted: 1'], index= ['Actual: 0', 'Actual: 1'])\n",
    "\n",
    "print(displayCMatrix)\n",
    "print()\n",
    "print(\"accuracy:\", accuracy)\n",
    "print(\"error:\", error)\n",
    "print(\"sensitivity:\", sensitivity)\n",
    "print(\"specificity:\", specificity)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
