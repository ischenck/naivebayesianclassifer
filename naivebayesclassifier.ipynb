{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ian Schenck\n",
    "# Victor Castellanos\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a file that uses , as delimiter\n",
    "def load_file(path, names):\n",
    "    if not path.is_file():\n",
    "        raise FileNotFoundError(str(path))\n",
    "        \n",
    "    data = pd.read_csv(path, sep=\",\", names=names, header=None)\n",
    "    return data\n",
    "\n",
    "# load data given\n",
    "def load_dfs():\n",
    "    cols = [\"PregnanciesNumber\", \"GlucosePlasma\", \"BloodPressureDiastolic\", \"SkinThicknessTriceps\", \n",
    "           \"Insulin2Hour\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"OutcomeClass\"]\n",
    "    path = Path.cwd() / \"data\"\n",
    "    diabetes_file = path / \"pima-indians-diabetes.data.csv\"\n",
    "    train_file = path / \"train.csv\"\n",
    "    test_file = path / \"test.csv\"\n",
    "\n",
    "    diabetes_data = load_file(diabetes_file, cols)\n",
    "    train_data = load_file(train_file, cols)\n",
    "    test_data = load_file(test_file, cols)\n",
    "    return diabetes_data, train_data, test_data\n",
    "\n",
    "# calculate the mean and standard deviation for each column, separated by class\n",
    "def mean_std_by_class(data):\n",
    "    data_by_class = data.groupby('OutcomeClass')\n",
    "    mean = data_by_class.mean()\n",
    "    std = data_by_class.std()\n",
    "    false_mean = mean[std.index == 0.0].values[0]\n",
    "    false_std = std[std.index == 0.0].values[0]\n",
    "    true_mean = mean[std.index == 1.0].values[0]\n",
    "    true_std = std[std.index == 1.0].values[0]\n",
    "    return false_mean, false_std, true_mean, true_std\n",
    "\n",
    "# calculate normal distribution likelihood\n",
    "def norm_dist(data, mean, std):\n",
    "    variance = std**2\n",
    "    denominator = (2 * math.pi* variance)**(.5)\n",
    "    numerator = np.exp(-(data - mean)**2 / (2 * variance))\n",
    "    return numerator / denominator\n",
    "\n",
    "# calculate class probabilities\n",
    "def class_probs(data):\n",
    "    n_false = train_data['OutcomeClass'][train_data['OutcomeClass'] == 0].count()\n",
    "    n_true = train_data['OutcomeClass'][train_data['OutcomeClass'] == 1].count()\n",
    "    n_total = train_data['OutcomeClass'].count()\n",
    "\n",
    "    p_false = n_false / n_total\n",
    "    p_true = n_true / n_total\n",
    "    \n",
    "    return p_false, p_true\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "diabetes_data, train_data, test_data = load_dfs()\n",
    "\n",
    "# calculate class probabilities\n",
    "p_false, p_true = class_probs(train_data)\n",
    "\n",
    "# calculate mean and std by class\n",
    "false_mean, false_std, true_mean, true_std = mean_std_by_class(train_data)\n",
    "\n",
    "test_data_no_outcome = test_data.drop('OutcomeClass', axis=1)\n",
    "\n",
    "# calculate probability for false\n",
    "false_norm = norm_dist(test_data_no_outcome, false_mean, false_std)\n",
    "false_norm = false_norm.prod(axis=1) * p_false\n",
    "\n",
    "# calculate probability for true\n",
    "true_norm = norm_dist(test_data_no_outcome, true_mean, true_std)\n",
    "true_norm = true_norm.prod(axis=1) * p_true\n",
    "\n",
    "# assign a predicted outcome to each patient\n",
    "norm = pd.concat([false_norm, true_norm], axis=1)\n",
    "norm['diabetes_predicted'] = np.where(norm[1] > norm[0], 1.0, 0.0)\n",
    "# compare predicted outcome to actual outcome and label as true or false\n",
    "merged = pd.concat([norm['diabetes_predicted'], test_data['OutcomeClass']], axis=1)\n",
    "merged['accurate'] = np.where(merged['diabetes_predicted'] == merged['OutcomeClass'], True, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7480314960629921"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "accuracy = merged.mean()['accurate']\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Predicted: 0  Predicted: 1\n",
      "Actual: 0           132            28\n",
      "Actual: 1            36            58\n",
      "\n",
      "accuracy: 0.7480314960629921\n",
      "error: 0.25196850393700787\n",
      "sensitivity: 0.6170212765957447\n",
      "specificity: 0.825\n"
     ]
    }
   ],
   "source": [
    "#adding column true negative, false negative, true positive, and false positive and labeling with 1 if label applies \n",
    "merged['TN'] = np.where((merged['diabetes_predicted']==0) & (merged['OutcomeClass']==0),1,0) \n",
    "merged['FN'] = np.where((merged['diabetes_predicted']==0) & (merged['OutcomeClass']==1),1,0)\n",
    "merged['FP'] = np.where((merged['diabetes_predicted']==1) & (merged['OutcomeClass']==0),1,0)\n",
    "merged['TP'] = np.where((merged['diabetes_predicted']==1) & (merged['OutcomeClass']==1),1,0)\n",
    "\n",
    "\n",
    "#making dataFrame with TN, FN, FP, TP only columns and find the sum of those columns\n",
    "confusion = merged.drop(['OutcomeClass','diabetes_predicted','accurate'],axis=1)\n",
    "confusion = confusion.sum(axis=0)\n",
    "#confusion\n",
    "\n",
    "#getting values from dataframe \n",
    "TN = confusion[0]\n",
    "FN = confusion[1]\n",
    "FP = confusion[2]\n",
    "TP = confusion[3]\n",
    "#calculating accuracy, error, senstivity and specificity\n",
    "#accuracy is the percent of times that we have predicted correct\n",
    "accuracy = (TP + TN)/(TP + FP + TN + FN)\n",
    "#error is the percent of times that we have predicted incorrect\n",
    "error = (FP + FN)/(TP + FP + TN + FN)\n",
    "#sensitivity tells us for each time that we predicted positive how many of those times were correct in this case 61.7 percent\n",
    "sensitivity = (TP)/(FN + TP)\n",
    "#specificity tells us for each time that we predicted negative how many of those times were we correct, in this case 82.5percent\n",
    "specificity = TN/(TN + FP)\n",
    "\n",
    "#making a datafram to dispaly confusion matrix\n",
    "displayCMatrix = [[TN,FP],[FN,TP]] \n",
    "displayCMatrix = pd.DataFrame(displayCMatrix, columns = ['Predicted: 0','Predicted: 1'], index= ['Actual: 0', 'Actual: 1'])\n",
    "\n",
    "print(displayCMatrix)\n",
    "print()\n",
    "print(\"accuracy:\", accuracy)\n",
    "print(\"error:\", error)\n",
    "print(\"sensitivity:\", sensitivity)\n",
    "print(\"specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our interpretation of these values is that sensitivity is more important than specificity in this particular case.  If someone does have diabetes, it could be life threatening for them to go undiagnoses.  So the fact that our sensitivity is much less than specificity is a negative aspect of this classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
